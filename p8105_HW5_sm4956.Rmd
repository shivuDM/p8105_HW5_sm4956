---
title: "p8105_hw5_sm4956"
author: Shivangi Deepak Mewada
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE)
```
Loading the libraries required for the midterm, configuring the code chunk dimensions, and creating themes and display for the plots for knit.


## Problem 0
* created repository and R project for HW5, created rmd file and rending to GitHub. 
* created a sub-directory/ data folder for the data set files to be used for this HW

```{r load libraries}
library(tidyverse)
library(readxl)
library(dplyr)
library(ggridges)
library(tibble)
library(broom)
library(purrr)
options(tibble.print_min = 5)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Importing data in individual spreadsheets from data subdirectory. The dataframe was created that includes list of all files in the directory. map used to import data using `read_csv` function; and unnest used.
```{r}
full_df = 
  tibble(
    files = list.files("data/zip_data/"),
    path = str_c("data/zip_data/", files)
  ) %>% 
  mutate(data = map(path, read_csv)) %>% 
  unnest(cols = c(data))
```

Tidying the data, manipulating file names to include control arm and subject ID, weekly observations tidied, with other data wrangling.
```{r}
tidy_df = 
  full_df %>% 
  mutate(
    files = str_replace(files, ".csv", ""),
    group = str_sub(files, 1, 3)) %>% 
  pivot_longer(
    week_1:week_8,
    names_to = "week",
    values_to = "outcome",
    names_prefix = "week_") %>% 
  mutate(week = as.numeric(week)) %>% 
  select(group, subj = files, week, outcome)
```

Spaghetti plot to show observations on each subject over time, and noting the trend
```{r}
tidy_df %>% 
  ggplot(aes(x = week, y = outcome, group = subj, color = group)) + 
  geom_point() + 
  geom_path() + 
  facet_grid(~group)
```

This plot suggests high within-subject correlation -- subjects who start above average end up above average, and those that start below average end up below average. Subjects in the control group generally don't change over time, but those in the experiment group increase their outcome in a roughly linear way. 

## Problem 2
Importing the raw homicides dataset:
```{r homicides raw dataset}
homicide_raw_ds = read_csv(
    "data/homicide-data.csv") %>%
  janitor::clean_names()
```
Describing the raw Homicides dataset:
- The total number of observations/rows are **`r nrow(homicide_raw_ds)`** and the total number of variables/columns are **`r ncol(homicide_raw_ds)`**
- This dataset has data on more than 52000 criminal homicides over the past decade in 50 of the largest American cities.This  included the location of the killing, whether an arrest was made and, in most cases, basic demographic information about each victim.
- The key variables in this data set are **`r colnames(homicide_raw_ds)`**. Some key variables include names of the homicide victims,their race, age, the cities and states these victims belonged to, and information about the criminal disposition, i.e., if there was no arrest, closed by arrest, or closed without arrest.

Data wrangling to create a city_state variable (e.g. “Baltimore, MD”), summarizing within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).
Removing Tulsa_AL because of the wrong combination of city and state.
```{r summarizing for homicides}
homicide_ds = homicide_raw_ds %>%
unite('city_state',"city":"state", remove = FALSE)%>%
  select(-city,-state)%>%
   mutate (homi_type = case_when (disposition == "Closed without arrest" ~ "Unsolved", 
                                  disposition == "Open/No arrest" ~ "Unsolved", 
                                  disposition == "Closed by arrest" ~ "Solved" )) %>%
  group_by(city_state) %>%
  filter (city_state != "Tulsa_AL") %>%
  count(homi_type) %>%
  spread(key = homi_type, value = n)%>%
  mutate (total = Solved + Unsolved) %>%
  select (-Solved)
  
```

Filtering for Baltimore:
```{r Baltimore homicides}
baltimore_homi = homicide_ds %>%
  filter(city_state == "Baltimore_MD")
```

Estimating Homicide proportion for Baltimore using prop.test and getting estimate and 95% confidence intervals using broom::tidy
```{r Baltimore proportion using prop.test}
baltimore_proportion = 
  prop.test(baltimore_homi$Unsolved, baltimore_homi$total)%>%
  broom::tidy() %>%
  select (estimate, conf.low, conf.high)

  knitr::kable(baltimore_proportion)
```

Estimating Homicide proportion for each city using prop.test and getting estimate and 95% confidence intervals using Tidy pipeline. 
```{r proportions for all cities}
cities_proportion = homicide_ds %>%
  mutate(city_prop = list(broom::tidy(prop.test(Unsolved, total, conf.level=0.95)))) %>%
  unnest(city_prop)%>%
  select (city_state, estimate, conf.low, conf.high)%>%
  ungroup()
 
```

Plotting for the proportion estimate and 95% CI for each city, using geom_errorbar to get the 95% CI on the plot, organizing the cities according to the proportion of unsolved homicides in order.
```{r plotting proportions for cities}
plot_cities_prop = cities_proportion %>%
  mutate (city_state = reorder (city_state, estimate)) %>%
  ggplot(aes (x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(legend.position = "bottom", axis.text.x = element_text(angle = 60, hjust = 1)) + 
  labs (title = "Plot for proportion estimate with 95% CI for Cities and states",
        x = "City and State",
        y = "Proportion estimate with 95% CI")
  
plot_cities_prop
```

## Problem 3
Generating 1 dataset for the normal distribution model x∼Normal[μ,σ] using function, using broom::tidy to get estimate and p-value for the one-sided t-test estimate and p-value: for μ = 0.
```{r}
p3_dist = function(n = 30, mu = 0, sigma = 5) {
  
  p3_data = tibble(
    x = rnorm(n, mean = mu, sd = sigma),
  )
  p3_data %>%
     t.test (mu = 0, conf.level = 0.95) %>%
      broom::tidy() %>%
    select(estimate, p.value)
}

```

Generating 5000 datasets from the model used above for μ=0
```{r creating datasets}
mu_5k = vector("list", 5000)
for(i in 1:5000) {
  mu_5k [[i]] = p3_dist()
}

mu_5k = mu_5k %>%
  bind_rows()
```
